{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "## Recurrent Neural Network Project\n",
    "## Project: Train a character level sequence generator\n",
    "\n",
    "Welcome to the Recurrent Neural Network Project in the Artificial Intelligence Nanodegree! In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'Implementation'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.  \n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "In this project you will implement a popuular Recurrent Neural Network (RNN) architecture to create an English language sequence generator capable of building semi coherent english sentences from scratch by building them up character-by-character.  This will require a substantial amount amount of parameter tuning on a large trainnig corpus (at least 100,000 characters long).  In particular for this project we will be using a complete version of Sir Arthur Conan Doyl's classic book The Adventures of Sherlock Holmes.\n",
    "\n",
    "The particular network architecture we will employ is known as  [Long Term Short Memory (LTSM)](https://en.wikipedia.org/wiki/Long_short-term_memory), which helps significantly avoid technical problems with optimization of RNNs.  \n",
    "\n",
    "**Note:** Tuning RNNs is a computationally intensive endevour and thus timely on a typical CPU.  Using a reasonable sized Amazon GPU can speed up training by a factor of 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and preprocessing a text dataset\n",
    "\n",
    "Our first task is to grab a large text corpus for use in training, and on it we perform a several light of pre-processing tasks.  The default corpus we will use is Sherlock Holmes, but you can use a variety of others as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this corpus has total length = 142562\n",
      "this corpus has = 46 number of unique characters\n",
      "The first 500 characters of the text are\n",
      "------------------------------------------------------------\n",
      " alice's adventures in wonderland  lewis carroll  the millennium fulcrum edition 3.0  chapter i  down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought alice `without pictures or conversation?' so she was considering in her own mind (as well as she could, for the hot day ma\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# grab text-based file from the web using keras's get_file command\n",
    "# path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "# path = get_file('alice.txt', origin=\"http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt\")\n",
    "path = get_file('sherlock.txt', origin=\"http://www.gutenberg.org/cache/epub/1661/pg1661.txt\")\n",
    "\n",
    "# read in the text, transforming everything to lower case\n",
    "text = open(path).read().lower()\n",
    "\n",
    "# remove all 'new line' tags and long blank spaces\n",
    "text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "text = text.replace('     ', ' ').replace('\\r', '')\n",
    "text = text.replace('   ', ' ').replace('\\r', '')\n",
    "text = text.replace('  ', ' ').replace('\\r', '')\n",
    "\n",
    "# count the number of unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# create dictionaries containing each character\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# print some of the text, as well as statistics\n",
    "print (\"this corpus has total length = \" +  str(len(text)))\n",
    "print (\"this corpus has = \" +  str(len(chars)) + \" number of unique characters\")\n",
    "print ('The first 500 characters of the text are')\n",
    "print ('------------------------------------------------------------')\n",
    "print (text[0:500])\n",
    "print ('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting our text into sequences\n",
    "\n",
    "Now we need to cut up the text into equal length sequences.  However it can certainly be the case that a word at the start or end of a sequence might get cut off, so in order to not lose this information we cut up the text in a simiilar manner to how images / audio are cut for classification - via *windowing*.  Imagine the entire text as one long string.  We slide a window of fixed length along the string from left to right - taking a step of a certain number of characters each time - and take a snapshot of whats in the window at each moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 47508\n",
      "Vectorization...\n",
      " alice's adventures in wonderland  lewis\n",
      "ice's adventures in wonderland  lewis ca\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "# print out what a few of the first segments look like\n",
    "print (sentences[0])\n",
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use Keras to quickly build a single hidden layer RNN - where our hidden layer consists of LTSM modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our RNN build we can now train our model on the input text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "47508/47508 [==============================] - 1188s - loss: 3.0738  \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"nce?\"' `thank you, it's a very interesti\"\n",
      "nce?\"' `thank you, it's a very interestitotooootoootoot tooooootoooooototo tootootoot totootootooooooootot bootooootototoooo tootomotot bootooooootooto toooooototooooooo to tootootoototoototot tooootootootoot otoouto tootootoo tooooootooootot toot tooot bootototoo toototoooot tootoootoooto tooootootot botootoot to toototooo tooot tototoooootootog tooooootoootoo tootoototototoootoootoooototot totoooototototoooo, tootoooooototot totoototo\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"nce?\"' `thank you, it's a very interesti\"\n",
      "nce?\"' `thank you, it's a very interestimo toout mot, bouo to, tootooo tumonto toom,moot oo boootootooboovoot oomoboot,ot t o,o, atobo, tob,onot boo botoy, totyo oo,o to bt motyt oototoomo tot,ooot t t oomton,ot ototot ot tt totoumooot yoootot boooto tosototong toooo toloot toot t bot, t boom botoumooom bot tototo latontto touooogootoouto too totomo tooogomo onootooo tog toog, bo te, bob bo bogo bt totomou,, botooonnnob boto t touuootoo\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"nce?\"' `thank you, it's a very interesti\"\n",
      "nce?\"' `thank you, it's a very interestig, mt t,tt,t bot ty t t,,, yon,yt toooo ty,otoote,oyobmogrya yg ou t m, t ogynlv thog,o tm n,gv b,,th, b ut,ooxtyon t,,,oggogaso lt,tyygevmt tywm, ti ty,,oo y, b t'oov bhubo, ab,,airyyy,io,obot,,utoohyuyotvt,, m,,ouoo, fo boyveog t, l l,omgh, th thont tyonttbm votobooyo,ogo,ot t ett tom tont youitourllon,ooo toubo tootltut,,t bot aby og, toub tolngot.abotodotab, tot,ot mous,om-' iy,o omo tayvy, tb\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"nce?\"' `thank you, it's a very interesti\"\n",
      "nce?\"' `thank you, it's a very interestinot ft!mg t tmt ououtogt gotoout,`m t,sufovoaumottob,l,,o uly byot,o:oouautoouo,uommonugutagtaomgo t?ooovionatyeooinfohvvoghyol ty,, nbumoob uvovov,,,foy,lovo tng-oomu montot,'l iot, lu, om,mbbt mytaog, vu,tobty toto-uoo'oooutt:taotuogt by km oodnd,hdlfaeat ioo, uv ooy,ta?y,r lo,og e!lnsonbogyum,t toooyovobyglabl, m,v y orog bos bm  yoou leotbf,obomg,ob, wo'vayotogdhutixubom ncoen,uout,m t otonf,k\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Epoch 1/1\n",
      "11392/47508 [======>.......................] - ETA: 924s - loss: 2.8066"
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
