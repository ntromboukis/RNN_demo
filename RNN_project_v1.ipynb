{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "## Recurrent Neural Network Project\n",
    "## Project: Train a character level sequence generator\n",
    "\n",
    "Welcome to the Recurrent Neural Network Project in the Artificial Intelligence Nanodegree! In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'Implementation'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.  \n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "In this project you will implement a popuular Recurrent Neural Network (RNN) architecture to create an English language sequence generator capable of building semi coherent english sentences from scratch by building them up character-by-character.  This will require a substantial amount amount of parameter tuning on a large trainnig corpus (at least 100,000 characters long).  In particular for this project we will be using a complete version of Sir Arthur Conan Doyl's classic book The Adventures of Sherlock Holmes.\n",
    "\n",
    "The particular network architecture we will employ is known as  [Long Term Short Memory (LTSM)](https://en.wikipedia.org/wiki/Long_short-term_memory), which helps significantly avoid technical problems with optimization of RNNs.  \n",
    "\n",
    "**Important note:** Tuning RNNs is a computationally intensive endevour and thus timely on a typical CPU.  Using a reasonable sized cloud-based GPU can speed up training by a factor of 10.  Also because of the long training time it is highly recommended that you carefully write the output of each step of your process to file.  This is so that all of your results are saved even if you close close the web browser you're working out of, as the processes will continue processing in the background but variables/output in the notebook system will not update when you open it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### A simple way to write output to file\n",
    "x = 2   \n",
    "f = open('my_test_output.txt', 'w')              # create an output file to write too\n",
    "f.write('this is only a test ' + '\\n')           # print some output text\n",
    "f.write('the value of x is ' + str(x) + '\\n')    # record a variable value\n",
    "f.close()                                        # close the file when everything is recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys\n",
    "f = open('RNN_seq_gen_output.txt', 'w')              # create an output file to write too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Downloading and preprocessing a text dataset\n",
    "\n",
    "Our first task is to grab a large text corpus for use in training, and on it we perform a several light of pre-processing tasks.  The default corpus we will use is the classic book Sherlock Holmes, but you can use a variety of others as well - so long as they are fairly large (around 100,000 characters or more).  \n",
    "\n",
    "The first part of pre-processing we need to do is to - after readin gin the text - convert everything to lower-case.  This is done in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the text, transforming everything to lower case\n",
    "text = open('holmes.txt').read().lower()\n",
    "text = text[:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets examine a bit of the raw text.  Because we are interested in creating sentences of English words automatically by building up each word character-by-character, we only want to train on valid English words.  In other words - we need to remove all of the other junk characters that aren't words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\xef\\xbb\\xbfproject gutenberg's the adventures of sherlock holmes, by arthur conan doyle\\r\\n\\r\\nthis ebook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  you may copy it, give it away or\\r\\nre-use it under the terms of the project gutenberg license included\\r\\nwith this ebook or online at www.gutenberg.net\\r\\n\\r\\n\\r\\ntitle: the adventures of sherlock holmes\\r\\n\\r\\nauthor: arthur conan doyle\\r\\n\\r\\nposting date: april 18, 2011 [ebook #1661]\\r\\nfirst posted: november 29, 2002\\r\\n\\r\\nlanguage: english\\r\\n\\r\\n\\r\\n*** start of this project gutenberg ebook the adventures of sherlock holmes ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nproduced by an anonymous project gutenberg volunteer and jose menendez\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nthe adventures of sherlock holmes\\r\\n\\r\\nby\\r\\n\\r\\nsir arthur conan doyle\\r\\n\\r\\n\\r\\n\\r\\n   i. a scandal in bohemia\\r\\n  ii. the red-headed league\\r\\n iii. a case of identity\\r\\n  iv. the boscombe valley mystery\\r\\n   v. the five orange pips\\r\\n  vi. the man with the twisted lip\\r\\n vii. the adventure of the blue carbuncle\\r\\nviii. t\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### print out the first 1000 characters of the raw text to get a sense of what we need to throw out\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - there's a lot of junk here!  e.g., all the carriage return and newline sequences '\\n' and '\\r'' sequences.  Lets remove these from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### find and replace '\\n' and '\\r' symbols - replacing them \n",
    "text = text.replace('\\n','')    # replacing '\\n' with '' simply removes the sequence\n",
    "text = text.replace('\\r','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how the first 1000 characters of our text looks now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\xef\\xbb\\xbfproject gutenberg's the adventures of sherlock holmes, by arthur conan doylethis ebook is for the use of anyone anywhere at no cost and withalmost no restrictions whatsoever.  you may copy it, give it away orre-use it under the terms of the project gutenberg license includedwith this ebook or online at www.gutenberg.nettitle: the adventures of sherlock holmesauthor: arthur conan doyleposting date: april 18, 2011 [ebook #1661]first posted: november 29, 2002language: english*** start of this project gutenberg ebook the adventures of sherlock holmes ***produced by an anonymous project gutenberg volunteer and jose menendezthe adventures of sherlock holmesbysir arthur conan doyle   i. a scandal in bohemia  ii. the red-headed league iii. a case of identity  iv. the boscombe valley mystery   v. the five orange pips  vi. the man with the twisted lip vii. the adventure of the blue carbuncleviii. the adventure of the speckled band  ix. the adventure of the engineer's thumb   x. the adventure \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### print out the first 1000 characters of the raw text to get a sense of what we need to throw out\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still looks like there's a few more non-English characters / character sequences to pull out.  Try to remove as many of these as you possibly can in the next Python cell.  You might print out more characters to get a better sense of things that need to be removed.\n",
    "\n",
    "Try to remove as many bad characters / sequences as you can see in the first characters of the text, but don't worry if you don't remove every last bad character / string.   The bulk of this text is made up of valid English words and our RNN will learn to produce real sentences from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: remove as many non-English characters and character sequences as you can \n",
    "# some of the non-english I see\n",
    "non_english = ['\\xef','\\xbb','\\xbf','*','#','[',']','i.','ii.','iii.','iv.','v.','vi.','v.','vi.','vii.','viii.','ix.','x.','0','1','2','3','4','5','6','7','8','9']\n",
    "for i in non_english:\n",
    "    text = text.replace(i,'')\n",
    "text = text.replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"project gutenberg's the adventures of sherlock holmes, by arthur conan doylethis ebook is for the use of anyone anywhere at no cost and withalmost no restrictions whatsoever. you may copy it, give it away orre-use it under the terms of the project gutenberg license includedwith this ebook or online at www.gutenberg.nettitle: the adventures of sherlock holmesauthor: arthur conan doyleposting date: april , ebook first posted: november , language: english start of this project gutenberg ebook the adventures of sherlock holmes produced by an anonymous project gutenberg volunteer and jose menendezthe adventures of sherlock holmesbysir arthur conan doyle  a scandal in bohemia i the red-headed league ii a case of identity  the boscombe valley mystery  the five orange pips v the man with the twisted lip vi the adventure of the blue carbunclevii the adventure of the speckled band  the adventure of the engineer's thumb  the adventure of the noble bachelor x the adventure of the beryl coronet xi \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### print out the first 1000 characters of the raw text to get a sense of what we need to throw out\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have thrown out a good number of non-English characters/character sequences lets print out some statistics about the dataset - including number of total characters and number of unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this corpus has 95161 total number of characters\n",
      "this corpus has 41 unique characters\n"
     ]
    }
   ],
   "source": [
    "# count the number of unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# print some of the text, as well as statistics\n",
    "print (\"this corpus has \" +  str(len(text)) + \" total number of characters\")\n",
    "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step:  convert our characters via a look up table into numerical values.  We can't just throw characters into any machine learning algorithm - they only ingest numerical values.  So we need to create a function that transforms each of our input characters into distinct numerical values - like integers.  To do this we make a simple dictionary mapping each unique character to a unique integer.  To re-translate the output of our RNN - which will be a sequence of integers - into our unique set of characters we also create the inverse function dictionary mapping integers back to our unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### generate function mapping each unique character to a unique integer, as well as its inverse\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))  # map each unique character to unique integer\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))  # map each unique integer back to unique character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting our text into sequences\n",
    "\n",
    "Now we need to cut up the text into equal length sequences.  However it can certainly be the case that a word at the start or end of a sequence might get cut off, so in order to not lose this information we cut up the text in a simiilar manner to how images / audio are cut for classification - via *windowing*.  Imagine the entire text as one long string.  We slide a window of fixed length along the string from left to right - taking a step of a certain number of characters each time - and take a snapshot of whats in the window at each moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 31707\n",
      "Vectorization...\n",
      "project gutenberg's the adventures of sh\n",
      "ject gutenberg's the adventures of sherl\n"
     ]
    }
   ],
   "source": [
    "### cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "# print out what a few of the first segments look like\n",
    "print (sentences[0])\n",
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setting up our RNN\n",
    "\n",
    "With our dataset loaded in and pre-processed we can now begin setting up our RNN.  We use Keras to quickly build a single hidden layer RNN - where our hidden layer consists of LTSM modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "### necessary functions from the keras library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its your turn to build a simple single-hidden layer RNN with LTSM hidden units, a softmax activation, and categorical_crossentropy loss function.  This can be constructed using just a few lines - see e.g., the [general Keras documentation](https://keras.io/getting-started/sequential-model-guide/) and the [LTSM documentation in particular](https://keras.io/layers/recurrent/) for examples of how to quickly use Keras to build neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### TODO build the required RNN model: a single LSTM hidden layer with softmax activation, categorical_crossentropy loss \n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our RNN build we can now train our model on the input text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sampling function for RNN-based predictions\n",
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) \n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('RNN_output.txt', 'w')              # create an output file to write too\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 50):\n",
    "    # print update to console\n",
    "    print()\n",
    "    print('-' * 40)\n",
    "    line = 'Iteration ' + str(iteration) + '\\n'\n",
    "    print(line)\n",
    "    \n",
    "    # record iteration count\n",
    "    f.write('-' * 40 + '\\n')\n",
    "    f.write(line)         \n",
    "    \n",
    "    # fit model to current batch\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    # print update to console and record\n",
    "    line = 'GENERATING WITHI SEED: \"' + sentence + '\"' + '\\n'\n",
    "    print(line)\n",
    "    f.write(line)\n",
    "    \n",
    "    # print generated sentece and record\n",
    "    print(generated + '\\n')\n",
    "    f.write(generated + '\\n')\n",
    "\n",
    "    # print predicted words\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "    # print out next character to command line\n",
    "    print(generated)\n",
    "    print('\\n')\n",
    "\n",
    "    # record next character\n",
    "    f.write(generated)\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
