{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks - introduction\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are the natural extention of feedforward networks to understanding the input / output relationship between ordered sequences.  In this short notebook we first motivate the use of RNNs by discussing some limitations of supervised learning in terms of the kinds of problems it can tackle.  We then describe the vanilla RNN framework more formally, as well as solution methods and implementation issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Limitations of supervised learning\n",
    "\n",
    "With supervised learning - regression and classification - we aim to learn the pattern between a set of fixed size input and ouptput data points.  For example, with object detection (a classification problem) we can use a classification algorithm to learn a model that distinguishes between image patches containing an object of interest (e.g., human face) and all those *not* containing this object.  This classification problem is shown figuratively in the image below (taken from [[1]](#bib_cell)).\n",
    "\n",
    "<img src=\"images/wright_bros_face_detect.png\" width=600 height=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - vector input / output\n",
    "\n",
    "Note one key aspect of this sort of problem we can solve with supervised learning: *the input and output of a supervised learning system are fixed length scalars / vectors.*  With object detection the inputs - image patches - are all the same size e.g., a 64 x 64 grid of pixels and the outputs (vectorized as a $64^2$ x 1 vector)  - labels - are integer valued scalars.  An example of a small image patch showing the value of each pixel (taken from [[2]](#bib_cell)) is shown below.\n",
    "\n",
    "<img src=\"images/sample_grid_a_square.png\" width=400 height=400/>\n",
    "\n",
    "But not all pattern recognition problems satisfy this condition on their input / output.  \n",
    "\n",
    "For example\n",
    "\n",
    "- An automatic speech recognition program takes in a sequence (a segment of raw audio) and outputs a sequence of letters (e.g., a word or sentence).  \n",
    "\n",
    "\n",
    "- An automatic lagnuage translator takes in a sequence of words in one language (e.g., English) and outputs a sequence of words in another language (e.g., Spanish). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Problem 2 - complicated sequential relationships \n",
    "\n",
    "A more subtle issue with pattern recognition tasks like e.g., machine translation is that the input / output data sequences have a more complicated relationship than that of a typical supervised learning problem.  Take an English to Spanish translation of the sentence\n",
    "\n",
    "I do not like cats. --> Los gatos me cae mal.\n",
    "\n",
    "If we look at this datapoint on a word-by-word level, then it is **not** the case that each word in the two sentences translates directly.  e.g., \"I\" does not translate correctly to \"Los\", \"do\" is not correctly translated as \"gatos\", etc.,  Moreover \"cats\" is near the back of the English sentence and near the front (\"los gatos\") of the Spanish translation.  So while on the whole these two sentences mean the same, it is not the case that each word can be correctly translated in sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    ">In summary, supervised learning cannot directly tackle pattern recognition problems whose input and output consist of ordered sequences of data.  Popular examples of such problems include speech recognition and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  RNN basic modeling\n",
    "\n",
    "RNNs are a direct generalization of feedforward networks to ordered sequential data.  This essentially allows us to apply the nonlinear power of feedforward neural nets to this sort of data.\n",
    "\n",
    "In this Section we introduce notation and formal modeling of the basic RNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1  Sequence notation\n",
    "\n",
    "First we need some notation to denote sequences of input / output.  We can denote one input sequence of data as \n",
    "\n",
    "$$\\mathbf{x}^{\\left(1\\right)},\\,\\mathbf{x}^{\\left(2\\right)},...,\\mathbf{x}^{\\left(S\\right)}$$\n",
    "\n",
    "Here each vector $\\mathbf{x}^{\\left(t\\right)}$\n",
    " is of length $L$.  Likewise each corresponding output sequence is denoted as \n",
    "\n",
    "$$\\mathbf{y}^{\\left(1\\right)},\\,\\mathbf{y}^{\\left(2\\right)},...,\\mathbf{y}^{\\left(S\\right)}$$\n",
    "\n",
    "and each output vector $\\mathbf{y}^{\\left(t\\right)}$ has length $K$.\n",
    "\n",
    "\n",
    "Notice here that while the input and output vectors themselves can have different lengths - $L$ and $K$ respectively - both input and output sequences are of length $S$.  The basic RNN can be adjusted to deal with input and output sequences of different lengths, which is something we discuss later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.2  From feedforward networks to RNNs \n",
    "\n",
    "RNNs are a natural extension of standard feedforward neural networks - where both input and output are ordered sequences.  \n",
    "\n",
    "Take a standard single (hidden) layer network with $M$ hidden units that takes as input vectors $\\bf{x}$.  The formula for each hidden unit is a nonlinear function like tanh, a logistic sigmoid, or a relu function, e.g., for a single tanh hidden unit the output with respect to one input datapoint can be written as $h_m$ where\n",
    "\n",
    "$$h_{m}=\\text{tanh}^{\\,}\\left(\\mathbf{x}^{T}\\mathbf{v}_{m}^{\\,}\\right)$$\n",
    "\n",
    "Abusing notation slightly, we can then write the entire output of this networks hidden layer in vector form as $\\bf{h}$\n",
    "\n",
    "$$\\mathbf{h}=\\text{tanh}\\left(\\mathbf{V}^{T}\\mathbf{x}^{\\,}\\right)$$\n",
    "\n",
    "\n",
    "Here $\\mathbf{V}$ is the matrix consisting of all weight vectors $\\mathbf{v}_1,...,\\mathbf{v}_M$ stacked together column-wise.\n",
    "\n",
    "Now the predicted output $\\hat{y}$ can be written as\n",
    "\n",
    "$$\\hat{y} = \\mathbf{u}^T\\mathbf{h}^{~} = \\sum_{m=1}^{M} u_{m}~\\text{tanh}^{\\,}\\left(\\mathbf{x}^{T}\\mathbf{v}_{m}^{\\,}\\right)$$\n",
    "\n",
    "How do we tune the parameters here to make sure our predicted output matches the real output?  By shoving the above through a cost function and tuning parameters so that the prediction $\\hat{y}$ closely matches its true value $y$.  Which cost function we choose depends on the application -e.g., for regression we want parameters chosen as to minimize\n",
    "\n",
    "$$g = (y - \\hat{y})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical representations \n",
    "\n",
    "This network is represented graphically in the left panel of the image below - note here the graph indicates the output is a vector of length 3.  \n",
    "\n",
    "In the right panel is a graphical representation of an RNN analog of this exact single hidden layer feedforward network.  Note that ostensibly the only difference is that with the analog RNN architecture the *hidden layer loops in on itself recursively*.  \n",
    "\n",
    "<img src=\"images/dnn2rnn.png\" width=500 height=500/>\n",
    "\n",
    "How many times does it loop in on itself?  *The hidden layer here loops in on itself the number of times equal to the number of elements in our input / output sequences*, which we have denoted as $S$.  \n",
    "\n",
    "SHOW IMAGE OF UNROLLED RNN TOO HERE\n",
    "\n",
    "Translating the picture formally, at the $s^{th}$ loop the hidden layer output $\\mathbf{h}_s$ is defined recursively in terms of $\\mathbf{h}_{s-1}$ as \n",
    "\n",
    "$$\\mathbf{h}_s=\\text{tanh}\\left(\\mathbf{V}^{T}\\mathbf{x}_s^{\\,} + \\mathbf{W}^T\\mathbf{h}_{s-1}\\right)$$\n",
    "\n",
    "and the associated output is then likewise given as \n",
    "\n",
    "$$\\hat{\\mathbf{y}}_{s} = \\mathbf{U}^T\\mathbf{h}_{s}^{~}$$\n",
    "\n",
    "And these are defined for each $s = 1,...,S$.  Once again we wish to tune parameters so that each of our predictions $\\hat{\\mathbf{y}}_{s}$ closely matches our actual output $\\mathbf{y}_{s}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the similarities between the equations that define a single hidden layer feedforward network, and those that define the analagous RNN.  We have literally just adjusted the feedforward architecture so reasonably so that we can deal with sequential input / output data!\n",
    "\n",
    "How do we tune the parameters here to make sure our predicted output sequence matches the real output sequence?  By shoving the above through a cost function!  Which cost function we choose depends on the application - e.g., if this is a regression or classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.  Parameter tuning using gradient descent\n",
    "\n",
    "As with feedforward networks we use gradient descent (a.k.a. backpropogation) to tune the parameters of an RNN.  However one big technical hurdle that arises when applying gradient descent to feedforward nets (using e.g., logistic sigmoid activation functions) - the *gradient vanishing problem* - arises as well with RNNs.  \n",
    "\n",
    "Using a different activation function - i.e., relu and its offspring - greatly allievates this issue with feedforward networks, and offer similar relief with RNNs.  However a more commonly applied remedy with RNNs is to slightly adjust the architecture of recursive RNN loop itself.  There are an array of alterations one can make to the architecture of the hidden layer to alleviate technical optimization issues, with the most popular being the [Long Term Short Memory (LTSM)](https://en.wikipedia.org/wiki/Long_short-term_memory) framework.  For a nice visual introduction to LTSMs check out [4].  \n",
    "\n",
    "However again, the main point here is that these modifications are largely inspired by and used to alleivate issues with applying gradient descent to tune RNNs.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Examples to flush out notation\n",
    "\n",
    "In this section we review a number of example applications of RNNs commonly used in practice.  These range from the (relatively) small practical problem of time series prediction  - where excellent results can be found using a minimal datasets - to larger practical problems like machine translation and speech recognition  - where (relatively) large datasets are typically required to achieve even moderate results - to toy problems like sequence generation - where regardless of the amount of data used results are typically fair at best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Time series prediction\n",
    "\n",
    "How much will the per-share price of Apple stock be one week from today? If we could accurately predict this we could make money in the stock market (buying or shorting the stock). How many hamburgers will McDonalds sell in the city of Chicago next week? An accurate prediction of this quantity could have drastic cost-saving reprocusions on the business's supply chain and logistics operations.\n",
    "\n",
    "Such prediction problems are often addressed by collecting a running history of the desired target in question (e.g., Apple's historical stock price or the number of hamburgers sold in the past), and then by leveraging the history said trend to accurately predict its future value. This is typically referred to as a time series problem, and one way of solving it is by employing RNNs.\n",
    "\n",
    "Shown in the figure below is the result of using an RNN to make 7-day predictions on a 140-day time period of historical Apple stock data.  In particular, the RNN is trained on the first 2/3 of the series (with it's corresponding training fit shown in blue) and then used to make predictions on the final 1/3 of the series (the test prediction, shown in red).\n",
    "\n",
    "<img src=\"images/apple_RNN_prediction.png\" width=600 height=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling time series\n",
    "\n",
    "In order to employ RNNs one treats the time series prediction as a **regression problem** whose inputs are sequences of historical data and outputs single future values of the series.  In particular, denoting our time series as the sequence of numbers\n",
    "\n",
    "$$y_{0},y_{1},y_{2},...,y_{P}$$\n",
    "\n",
    "Then to phrase time series as a regression problem we define a window of size $T$ and repeatedly use $T$ previous values to predict the $T+1$ value.  For example using a window of size $T = 3$ we can create the following set of input/output pairs for applying a regression framework.\n",
    "\n",
    "\n",
    "$$\\begin{array}{c|c}\n",
    "\\text{Input} & \\text{Output}\\\\\n",
    "\\hline \\left[y_{0},y_{1},y_{2}\\right] & y_{3}\\\\\n",
    "\\left[y_{1},y_{2},y_{3}\\right] & y_{4}\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "\\left[y_{P-3},y_{P-2},y_{P-1}\\right] & y_{P}\n",
    "\\end{array}$$\n",
    "\n",
    "\n",
    "We then use a regressor to learn the general relationship between the input / output above, that is we tune the parameters of a regressor $f$ so that\n",
    "\n",
    "$$f\\left(\\left[y_{p-3},y_{p-2},y_{p-1}\\right]\\right)\\approx y_{p}$$\n",
    "\n",
    "holds for all $p=1,...,P$ where the approximation '$\\approx$' is desired to be as tight as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting time series data for RNN usage\n",
    "\n",
    "We can directly apply our RNNs to the time series problem.  We form a dataset of input / output pairs precisely as shown above!  Using the sequence notation from Section 2, our input sequence takes the form \n",
    "\n",
    "$$\\begin{array}{c|c}\n",
    "\\text{Input} & \\text{Output}\\\\\n",
    "\\hline \\mathbf{x}^{\\left(1\\right)}=\\left[y_{0},y_{1},y_{2}\\right]\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, & \\mathbf{y}^{\\left(1\\right)}=y_{3}\\\\\n",
    "\\mathbf{x}^{\\left(2\\right)}=\\left[y_{1},y_{2},y_{3}\\right]\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, & \\mathbf{y}^{\\left(2\\right)}=y_{4}\\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "\\mathbf{x}^{\\left(P-3\\right)}=\\left[y_{P-3},y_{P-2},y_{P-1}\\right] & \\mathbf{y}^{\\left(P-3\\right)}=y_{P}\n",
    "\\end{array}$$\n",
    "\n",
    "That is each element of the input sequence is a vector of length $T$, while each element of the output sequence is simply a scalar value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  Machine translation  \n",
    "\n",
    "Machine translation allows us to translates sentences from English to Spanish (or from any one language to another language). For example, you can use a machine translator to automatically translate the English sentence \"I do not like cats.\" to equivalent Spanish sentence \"Los gatos me cae mal.\".\n",
    "\n",
    "Note how doing this requires more than simply translating each individual word in the first sentence directly, e.g., the word 'cats' lies at the end of the English version but at the beginning (\"Los gatos\") of the Spanish version.  In other words, machine translation requires careful analysis of entire sentences or phrases in order to produce accurate translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting sentences for machine translation via RNNs\n",
    "\n",
    "In machine translation - and other text-based pattern recognition problems - each word is often represented in a 'one-hot encoding' format.  Take our example English-to-Spanish translation sentence\n",
    "\n",
    "I do not like cats. --> Los gatos me cae mal.\n",
    "\n",
    "Each word in the first sentnece is transformed into a long vector with length equal to the number total words in the English dictionary (so this vector can have tens of thousands of entries) ordered alphabetically.  Take the second word in the English sentence - \"do\".  The one-hot-encoded vector version of this word is all zeros except where the word \"do\" appears in the dictionary alphabetically (where a 1 is placed).  So e.g., if \"do\" were the 5,000th word in the dictionary the vector version would be all zeros with a 1 in the 5,000th entry.  This is illustrated figuratively below\n",
    "\n",
    "$\\mathbf{x}^{\\left(2\\right)}=\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\\right]\\begin{array}{c}\n",
    "\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\longleftarrow\\text{index of where \"do\" is in the English dictionary}\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\end{array}$\n",
    "\n",
    "\n",
    "Doing this for each word in the english sentence we get a sequence of 5 input vectors of equal length.\n",
    "\n",
    "$$\\mathbf{x}^{\\left(1\\right)},\\,\\mathbf{x}^{\\left(2\\right)},...,\\mathbf{x}^{\\left(5\\right)}$$\n",
    "\n",
    "We perform a similar one-hot-encoding of the output: the Spanish words.  However because the Spanish dictionary may be of different length so too can these output vectors be of different length than the input.  In either case, after performing the one-hot-encoding transformation we have a sequence of 5 output vectors as well.\n",
    "\n",
    "\n",
    "$$\\mathbf{y}^{\\left(1\\right)},\\,\\mathbf{y}^{\\left(2\\right)},...,\\mathbf{y}^{\\left(5\\right)}$$\n",
    "\n",
    "And using these sorts of encodings applied to a very large number of English / Spanish sentence pairs we can apply an RNN to learn a correspondence between the two languages that provides accurate English-to-Spanish translation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.3 Sspeech recognition\n",
    "\n",
    "Below is a picture of the waveform for the word \"dog\" spoken by a native English speaker. \n",
    "\n",
    "<img src=\"images/dog_speech.png\" width=600 height=600/>\n",
    "\n",
    "Such a waveform is typically windowed and transformed into a sequence of spectral slices - called a spectrogram - as illustrated figuratively in the image below (taken from [[1]](#bib_cell)).\n",
    "\n",
    "<img src=\"images/spectrogram_creation.png\" width=600 height=600/>\n",
    "\n",
    "Each spectral slice is an input vector to the speech recognition system - and all have the same dimension.  Each spectral slice has a corresponding output - a subcomponent sound of the word \"dog\" - called a phoneme.  \n",
    "\n",
    "In this manner speech recognition has an input / output sequence correspondence between the spoken waveform and the word dog. This sort of input / output correspondence holds more generally (for other words, sentences, languages) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting data for speech recognition\n",
    "\n",
    "UNDER CONSTRUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Sequence generation\n",
    "\n",
    "Generating text automatically.\n",
    "\n",
    "UNDER CONSTRUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bib_cell'></a>\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "[1] Watt, Jeremy et al. [Machine Learning Refined](www.mlrefined.com). Cambridge University Press, 2016\n",
    "\n",
    "[2] Image taken from http://pippin.gimp.org/image_processing/chap_dir.html\n",
    "\n",
    "[3] Graves, Alex. \"Supervised sequence labelling.\" Supervised Sequence Labelling with Recurrent Neural Networks. Springer Berlin Heidelberg, 2012. 5-13.\n",
    "\n",
    "[4] http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
