{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1.  Recurrent Neural Networks - introduction\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are the natural extention of feedforward networks to data consisting of ordered sequential input and output.  In this short notebook we first motivate the use of RNNs by discussing some limitations of supervised learning in terms of the kinds of problems it can tackle.  We then describe the vanilla RNN framework more formally, as well as solution methods and implementation issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Limitations of supervised learning\n",
    "\n",
    "With supervised learning - regression and classification - we aim to learn the pattern between a set of fixed size input and ouptput data points.  For example, with object detection (a classification problem) we can use a classification algorithm to learn a model that distinguishes between image patches containing an object of interest (e.g., human face) and all those *not* containing this object.  This classification problem is shown figuratively in the image below (used with permission from [[1]](#bib_cell).\n",
    "\n",
    "<img src=\"images/wright_bros_face_detect.png\" width=600 height=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - vector input / output\n",
    "\n",
    "Note one key aspect of this sort of problem we can solve with supervised learning: *the input and output of a supervised learning system are fixed length scalars / vectors.*  With object detection the inputs - image patches - are all the same size e.g., a 64 x 64 grid of pixels and the outputs (vectorized as a $64^2$ x 1 vector)  - labels - are integer valued scalars.  An example of a small image patch showing the value of each pixel (taken from [[2]](#bib_cell)) is shown below.\n",
    "\n",
    "<img src=\"images/sample_grid_a_square.png\" width=400 height=400/>\n",
    "\n",
    "But not all pattern recognition problems satisfy this condition on their input / output.  \n",
    "\n",
    "For example\n",
    "\n",
    "- An automatic speech recognition program takes in a sequence (a segment of raw audio) and outputs a sequence of letters (e.g., a word or sentence).  \n",
    "\n",
    "\n",
    "- An automatic lagnuage translator takes in a sequence of words in one language (e.g., English) and outputs a sequence of words in another language (e.g., Spanish). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Problem 2 - complicated sequential relationships \n",
    "\n",
    "A more subtle issue with pattern recognition tasks like e.g., machine translation is that the input / output data sequences have a more complicated relationship than that of a typical supervised learning problem.  Take an English to Spanish translation of the sentence\n",
    "\n",
    "I do not like cats. --> Los gatos me cae mal.\n",
    "\n",
    "If we look at this datapoint on a word-by-word level, then it is **not** the case that each word in the two sentences translates directly.  e.g., \"I\" does not translate correctly to \"Los\", \"do\" is not correctly translated as \"gatos\", etc.,  Moreover \"cats\" is near the back of the English sentence and near the front (\"los gatos\") of the Spanish translation.  So while on the whole these two sentences mean the same, it is not the case that each word can be correctly translated in sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    ">In summary, supervised learning cannot directly tackle pattern recognition problems whose input and output consist of ordered sequences of data.  Popular examples of such problems include speech recognition and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  RNN basic modeling\n",
    "\n",
    "RNNs are a direct generalization of feedforward networks to ordered sequential data.  In this Section we introduce notation and formal modeling of the basic RNN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1  Sequence notation\n",
    "\n",
    "First we need some notation to denote sequences of input / output.  We can denote one input sequence of data as \n",
    "\n",
    "$\\mathbf{x}^{\\left(1\\right)},\\,\\mathbf{x}^{\\left(2\\right)},...,\\mathbf{x}^{\\left(T\\right)}$\n",
    "\n",
    "Here each vector $\\mathbf{x}^{\\left(t\\right)}$\n",
    " is of length $N$.  Likewise each corresponding output sequence is denoted as \n",
    "\n",
    "$\\mathbf{y}^{\\left(1\\right)},\\,\\mathbf{y}^{\\left(2\\right)},...,\\mathbf{y}^{\\left(T\\right)}$\n",
    "\n",
    "and each output vector $\\mathbf{y}^{\\left(t\\right)}$ has length $M$.\n",
    "\n",
    "\n",
    "Notice here that while the input and output vectors themselves can have different lengths - $N$ and $M$ respectively - both input and output sequences are of length $T$.  The basic RNN can be adjusted to deal with input and output sequences of different lengths, which is something we discuss later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ \n",
    "### Example -  one-hot-encoding vectors for machine translation\n",
    "\n",
    "In machine translation - and other text-based pattern recognition problems - each word is often represented in a 'one-hot encoding' format.  Take our example English-to-Spanish translation sentence\n",
    "\n",
    "I do not like cats. --> Los gatos me cae mal.\n",
    "\n",
    "Each word in the first sentnece is transformed into a long vector with length equal to the number total words in the English dictionary (so this vector can have tens of thousands of entries) ordered alphabetically.  Take the second word in the English sentence - \"do\".  The one-hot-encoded vector version of this word is all zeros except where the word \"do\" appears in the dictionary alphabetically (where a 1 is placed).  So e.g., if \"do\" were the 5,000th word in the dictionary the vector version would be all zeros with a 1 in the 5,000th entry.  This is illustrated figuratively below\n",
    "\n",
    "$\\mathbf{x}^{\\left(2\\right)}=\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\\right]\\begin{array}{c}\n",
    "\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\longleftarrow\\text{index of where \"do\" is in the English dictionary}\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\end{array}$\n",
    "\n",
    "\n",
    "Doing this for each word in the english sentence we get a sequence of 5 input vectors of equal length.\n",
    "\n",
    "We perform a similar one-hot-encoding of the output: the Spanish words.  However because the Spanish dictionary may be of different length so too can these output vectors be of different length than the input.  In either case, after performing the one-hot-encoding transformation we have a sequence of 5 output vectors as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------ \n",
    "### Example -  speech recognition\n",
    "\n",
    "In machine translation - and other text-based pattern recognition problems - each word is often represented in a 'one-hot encoding' format.  Take our example English-to-Spanish translation sentence\n",
    "\n",
    "I do not like cats. --> Los gatos me cae mal.\n",
    "\n",
    "Each word in the first sentnece is transformed into a long vector with length equal to the number total words in the English dictionary (so this vector can have tens of thousands of entries) ordered alphabetically.  Take the second word in the English sentence - \"do\".  The one-hot-encoded vector version of this word is all zeros except where the word \"do\" appears in the dictionary alphabetically (where a 1 is placed).  So e.g., if \"do\" were the 5,000th word in the dictionary the vector version would be all zeros with a 1 in the 5,000th entry.  This is illustrated figuratively below\n",
    "\n",
    "$\\mathbf{x}^{\\left(2\\right)}=\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{array}\\right]\\begin{array}{c}\n",
    "\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\longleftarrow\\text{index of where \"do\" is in the English dictionary}\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\end{array}$\n",
    "\n",
    "\n",
    "Doing this for each word in the english sentence we get a sequence of 5 input vectors of equal length.\n",
    "\n",
    "We perform a similar one-hot-encoding of the output: the Spanish words.  However because the Spanish dictionary may be of different length so too can these output vectors be of different length than the input.  In either case, after performing the one-hot-encoding transformation we have a sequence of 5 output vectors as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bib_cell'></a>\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "[1] Watt, Jeremy et al. [Machine Learning Refined](www.mlrefined.com). Cambridge University Press, 2016\n",
    "\n",
    "[2] Image taken from http://pippin.gimp.org/image_processing/chap_dir.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
